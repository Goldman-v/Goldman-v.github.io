<!DOCTYPE html>
<html lang="zh-CN">

<title>Description</title>

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- 引用自己的库 -->
    <link rel="stylesheet" href="../../resource/css/cite.css">
    <script src="../../resource/js/cite.js"></script>
    <link rel="shortcut icon" href="../../resource/img/eeg-icon-black.svg">

    <!-- 3.6.0最新 -->
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script>

    <!-- bootstrap 4.6.0 -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js"></script>


    <!-- tocbot 4.12.3 -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.12.3/dist/tocbot.css">
    <script src="https://cdn.jsdelivr.net/npm/tocbot@4.12.3/dist/tocbot.min.js"></script>

</head>


<body style="font-family: '微软雅黑';background-color: rgb(245,247,255);">

    <div class="page">
        <div class="top">
            <div class="logo">
                <a href="../../index.html">
                    <img src="../../resource/img/logo.png" alt="智能感知交互实验室">
                </a>
            </div>

            <!-- <br> -->
        </div>
        <!-- 导航条 -->
        <nav class="site-header sticky-top py-0">
            <div class="container d-flex flex-column flex-md-row justify-content-between">
                <a class="py-2 d-none d-md-inline-block" href="../../index.html">首页</a>
                <a class="py-2 d-none d-md-inline-block" href="index.html">Home</a>
                <a class="py-2 d-none d-md-inline-block" href="download.html">Download</a>
                <a class="py-2 d-none d-md-inline-block" href="readme.html">ReadMe</a>
            </div>
        </nav>
        <div class="container">
            <div class="row">
                <div class="col-2">

                </div>
                <div class="col-8 js-toc-content" style="font-size: 1em;">
                    <br>
                    <h2 style="color: #1293EE; ">
                        Stimuli and Experiment
                    </h2>
                    <br>
                    We used the 48-question from Raven Progressive Matrices (RPM) as the subject's visual stimuli. In
                    addition, to ensure
                    subjects are not distracted as much as possible during the task time, we set answer time for
                    questions(15s) shorter than
                    the average time to work out the questions. The procedure of the experiment is shown in Fig.1
                    <img src="./resource/img/1-experiment procedure.png" class="img-fluid" style="padding-top:10px ;">
                    <p style="text-align: center;padding-top: 8px;">Fig. 1. The procedure of experiment </p>
                    Before each subject starts the investigation, first, the eye tracker is calibrated the gaze of the
                    current subject to
                    reduce the gaze error of the collection process. Then ten landscape pictures help the subject calm
                    down and enter the
                    test state. After that, it begins the test, which includes 48 questions. We set each question to
                    have a maximum
                    answering time of 15 seconds. The subject must press the button corresponding to the answer option
                    preset by the program
                    within the answering time to complete the question and automatically jump to the next question. If
                    the subject does not
                    meet the answer within the set time, it will automatically jump to the next question. Before jumping
                    to the next
                    question, a calibration break is set to ensure high-quality gaze data. It is a fixation point that
                    appeared in the
                    center of the screen for 0.5 seconds, and subjects were asked to focus on this fixation point as
                    much as possible.
                    During the answering process, the program simultaneously records the facial video, screen video, and
                    eye movement data
                    of each subject during completing the 48 reasoning questions.
                    <img src="./resource/img/2-Experiment setting.png" class="img-fluid" style="padding-top:10px ;">
                    <p style="text-align: center;padding-top: 8px;">Fig. 2. Experiment setting</p>
                    The experiment setting is shown in Fig. 2, and the visual stimuli materials are displayed on a
                    27-Inch monitor. The
                    subjects’ videos are captured from a commercial webcam fixed on the top of the screen. Logitech C270
                    HD Webcam is
                    chosen, since it is one of low-cost and most widely used webcam, supporting 1280 × 720 video
                    recording with
                    lightcorrection technology.
                    <br>
                    <br>
                    <h2 style=" color: #1293EE;">
                        Dataset Summary
                    </h2>
                    We collected gaze data from a total of 34 subjects(18 female and 16 male), with 22 wearing glasses
                    and 12 having normal
                    vision. Three subjects' gaze data were screened out due to improper experimental operation. The data
                    were collected
                    under well-lit indoor conditions, where the sampling rate of face video was 30Hz, and the sampling
                    rate of Tobii Pro
                    Nano eye tracker was 60Hz. The time for each subject to complete RMP test is about 8-11 minutes. The
                    entire dataset
                    contains 31 subjects' facial videos with a total length of 309 minutes. There are 556,476 images in
                    total, some face
                    image examples are shown in Fig.3.

                    <img src="./resource/img/3-face image.png" class="img-fluid" style="padding-top:10px ;">
                    <p style="text-align: center;padding-top: 8px;">Fig. 3. Face image from the dataset</p>

                    <img src="./resource/img/4-densityHeatMap.png" class="img-fluid" style="padding-top:10px ;">
                    <p style="text-align: center;padding-top: 8px;">Fig. 4. Gaze angle and gaze point distribution</p>
                    The angle range of the horizontal direction is (-10°, +10°), and the angle of the vertical direction
                    is (-12°, 12°),
                    which is smaller than another gaze dataset. It also means that RavenGaze places higher demands on
                    the recognition
                    algorithms.
                    <br>
                    <br>
                    <br>
                </div>

                <div class="col-mb-12 col-2 kit-hidden-tb post-toc-content">
                    <div id="toc-container" class="post-toc">
                        <div class="toc"></div>
                    </div>
                </div>
            </div>
        </div>



        <footer class="bottomPage container-fluid">
            <div class="col-md-10 bottomContent">
                <div class="col-md-5">
                    <p>邮箱：xutao@nwpu.edu.cn</p>
                    <p>地址：西安市长安区东祥路1号</p>
                </div>
                <div style="text-align: center;float: inline-end;" class="col-md-6">
                    <small class="d-block mb-3 text-muted">Intelligent Interaction Laboratory @ NWPU</small>

                    <small class="d-block mb-3 text-muted">All Rights Reserved © 2019-<div id="year"
                            style="display: inline-block">LastYear
                        </div></small>
                </div>
            </div>
        </footer>
    </div>




</body>

<script language="javascript" type="text/javascript">
    add_head_id();

    tocify();

    footer_year();
</script>


</html>